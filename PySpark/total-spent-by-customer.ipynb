{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"SpendByCustomer\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def extractCustomerPricePairs(line):\n",
    "    fields = line.split(',')\n",
    "    return (int(fields[0]), float(fields[2]))\n",
    "\n",
    "input = sc.textFile(\"data/customer-orders.csv\")\n",
    "mappedInput = input.map(extractCustomerPricePairs)\n",
    "totalByCustomer = mappedInput.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "results = totalByCustomer.collect();\n",
    "for result in results[:5]:\n",
    "    print(result)\n",
    "\n",
    "sc.stop()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(44, 4756.8899999999985)\n",
      "(35, 5155.419999999999)\n",
      "(2, 5994.59)\n",
      "(47, 4316.299999999999)\n",
      "(29, 5032.529999999999)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Sorted\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"SpendByCustomerSorted\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def extractCustomerPricePairs(line):\n",
    "    fields = line.split(',')\n",
    "    return (int(fields[0]), float(fields[2]))\n",
    "\n",
    "input = sc.textFile(\"data/customer-orders.csv\")\n",
    "mappedInput = input.map(extractCustomerPricePairs)\n",
    "totalByCustomer = mappedInput.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "#Changed for Python 3 compatibility:\n",
    "#flipped = totalByCustomer.map(lambda (x,y):(y,x))\n",
    "flipped = totalByCustomer.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "totalByCustomerSorted = flipped.sortByKey()\n",
    "\n",
    "results = totalByCustomerSorted.collect()[:5];\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "\n",
    "sc.stop()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3309.38, 45)\n",
      "(3790.570000000001, 79)\n",
      "(3924.230000000001, 96)\n",
      "(4042.6499999999987, 23)\n",
      "(4172.289999999998, 99)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Sorted DF\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TotalSpentByCustomer\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Create schema when reading customer-orders\n",
    "customerOrderSchema = StructType([ \\\n",
    "                                  StructField(\"cust_id\", IntegerType(), True),\n",
    "                                  StructField(\"item_id\", IntegerType(), True),\n",
    "                                  StructField(\"amount_spent\", FloatType(), True)\n",
    "                                  ])\n",
    "\n",
    "# Load up the data into spark dataset\n",
    "customersDF = spark.read.schema(customerOrderSchema).csv(\"data/customer-orders.csv\")\n",
    "\n",
    "totalByCustomer = customersDF.groupBy(\"cust_id\").agg(func.round(func.sum(\"amount_spent\"), 2) \\\n",
    "                                      .alias(\"total_spent\"))\n",
    "\n",
    "totalByCustomerSorted = totalByCustomer.sort(\"total_spent\")\n",
    "\n",
    "totalByCustomerSorted.show(totalByCustomerSorted.count())\n",
    "\n",
    "spark.stop()\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
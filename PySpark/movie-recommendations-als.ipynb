{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.recommendation import ALS, Rating\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"ml-100k/u.ITEM\", encoding='ascii', errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"MovieRecommendationsALS\")\n",
    "sc = SparkContext(conf = conf)\n",
    "sc.setCheckpointDir('checkpoint')\n",
    "\n",
    "print(\"\\nLoading movie names...\")\n",
    "nameDict = loadMovieNames()\n",
    "\n",
    "data = sc.textFile(\"file:///SparkCourse/ml-100k/u.data\")\n",
    "\n",
    "ratings = data.map(lambda l: l.split()).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2]))).cache()\n",
    "\n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "print(\"\\nTraining recommendation model...\")\n",
    "rank = 10\n",
    "# Lowered numIterations to ensure it works on lower-end systems\n",
    "numIterations = 6\n",
    "model = ALS.train(ratings, rank, numIterations)\n",
    "\n",
    "userID = int(sys.argv[1])\n",
    "\n",
    "print(\"\\nRatings for user ID \" + str(userID) + \":\")\n",
    "userRatings = ratings.filter(lambda l: l[0] == userID)\n",
    "for rating in userRatings.collect():\n",
    "    print (nameDict[int(rating[1])] + \": \" + str(rating[2]))\n",
    "\n",
    "print(\"\\nTop 10 recommendations:\")\n",
    "recommendations = model.recommendProducts(userID, 10)\n",
    "for recommendation in recommendations:\n",
    "    print (nameDict[int(recommendation[1])] + \\\n",
    "        \" score \" + str(recommendation[2]))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DF\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    # CHANGE THIS TO THE PATH TO YOUR u.ITEM FILE:\n",
    "    with codecs.open(\"E:/SparkCourse/ml-100k/u.ITEM\", \"r\", encoding='ISO-8859-1', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ALSExample\").getOrCreate()\n",
    "    \n",
    "moviesSchema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "    \n",
    "names = loadMovieNames()\n",
    "    \n",
    "ratings = spark.read.option(\"sep\", \"\\t\").schema(moviesSchema) \\\n",
    "    .csv(\"file:///SparkCourse/ml-100k/u.data\")\n",
    "    \n",
    "print(\"Training recommendation model...\")\n",
    "\n",
    "als = ALS().setMaxIter(5).setRegParam(0.01).setUserCol(\"userID\").setItemCol(\"movieID\") \\\n",
    "    .setRatingCol(\"rating\")\n",
    "    \n",
    "model = als.fit(ratings)\n",
    "\n",
    "# Manually construct a dataframe of the user ID's we want recs for\n",
    "userID = int(sys.argv[1])\n",
    "userSchema = StructType([StructField(\"userID\", IntegerType(), True)])\n",
    "users = spark.createDataFrame([[userID,]], userSchema)\n",
    "\n",
    "recommendations = model.recommendForUserSubset(users, 10).collect()\n",
    "\n",
    "print(\"Top 10 recommendations for user ID \" + str(userID))\n",
    "\n",
    "for userRecs in recommendations:\n",
    "    myRecs = userRecs[1]  #userRecs is (userID, [Row(movieId, rating), Row(movieID, rating)...])\n",
    "    for rec in myRecs: #my Recs is just the column of recs for the user\n",
    "        movie = rec[0] #For each rec in the list, extract the movie ID and rating\n",
    "        rating = rec[1]\n",
    "        movieName = names[movie]\n",
    "        print(movieName + str(rating))\n",
    "        \n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  1m\n",
    "\n",
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.recommendation import ALS, Rating\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"ml-1m/movies.dat\", encoding='ascii', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            fields = line.split('::')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"MovieRecommendationsALS\")\n",
    "sc = SparkContext(conf = conf)\n",
    "sc.setCheckpointDir('checkpoint')\n",
    "\n",
    "print(\"\\nLoading movie names...\")\n",
    "nameDict = loadMovieNames()\n",
    "\n",
    "data = sc.textFile(\"file:///E:/SparkCourse/ml-1m/ratings.dat\")\n",
    "\n",
    "ratings = data.map(lambda l: l.split(\"::\")).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2]))).cache()\n",
    "\n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "print(\"\\nTraining recommendation model...\")\n",
    "rank = 10\n",
    "numIterations = 20\n",
    "model = ALS.train(ratings, rank, numIterations)\n",
    "\n",
    "userID = int(sys.argv[1])\n",
    "\n",
    "print(\"\\nRatings for user ID \" + str(userID) + \":\")\n",
    "userRatings = ratings.filter(lambda l: l[0] == userID)\n",
    "for rating in userRatings.collect():\n",
    "    print(nameDict[int(rating[1])] + \": \" + str(rating[2]))\n",
    "\n",
    "print(\"\\nTop 10 recommendations:\")\n",
    "recommendations = model.recommendProducts(userID, 10)\n",
    "for recommendation in recommendations:\n",
    "    print(nameDict[int(recommendation[1])] + \\\n",
    "        \" score \" + str(recommendation[2]))\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}